\chapter{Clusterify}
	Clusterify è un'applicazione web che offre agli utenti la possibilità di leggere gli stessi tweet che leggerebbero sul loro \emph{newsfeed}, organizzati, però, in insiemi definiti dinamicamente in modo da permettere loro di soffermarsi solamente su testi appartenenti a categorie che questi reputano interessanti. L'idea di quest'applicazione è nata dal bisogno di non voler leggere tutti i tweet che una determinata persona pubblica. 

	Supponiamo che un utente $X$, aspirante cuoco, segua $Y$, cuoca neo-mamma conosciuta a livello nazionale. Quest'ultima pubblica due tweet:
	\begin{inparaenum}[\itshape 1\upshape)]
		\item \#Plumcake alla \#Nutella: continuano gli esperimenti @GialloZafferano ;)
		\item Giocare una battaglia a \#palledineve e perdere con il proprio piccolo di 3 anni <3
	\end{inparaenum}.
	Probabilmente $X$ sarà molto interessato alla ricetta del \emph{Plumcake} ma, al contempo, non attratto dalla vita personale di $Y$.

	Clusterify dividerà questi tweet in gruppi distinti ed $X$ avrà la possibilità di leggere solo quelli riferiti ad ambiti culinari senza essere disturbato da storie di vita personale, recensioni di film e quant'altro.

\section{Background}
	Clusterify nasce con lo scopo di raggruppare per argomenti i tweet presenti nel newsfeed di un qualsiasi utente. Per questo scopo è necessario attribuire ad ogni tweet gli argomenti di cui tratta raggruppandoli per quanto questi siano collegati tra di loro. 

	Si è deciso di utilizzare la \emph{Twitter API} per ottenere dati da twitter e \emph{dataTXT} per estrarre gli argomenti e calcolarne le varie relazioni.

	\input{twitter}
	\input{datatxt}

\section{Backend}
	Clusterify è un'applicazione Django -- \url{https://www.djangoproject.com/} -- pensata e creata basandosi sulla necessità di essere estesa e modellata secondo le preferenze degli sviluppatori. %inserire nuovi \emph{reader} e nuovi algoritmi di clustering. 

	Si può vedere il tutto come una struttura a mattoncini dove si possono scambiare dei blocchi per raggiungere il risultato atteso, senza necessariamente modificare il codice del \emph{core}. Questi blocchi sono i \emph{reader}, i \emph{livelli di cache} e gli \emph{algoritmi di cluster}.
	
	Permettere di ottenere testi da più fonti significa creare altrettanti mezzi per interfacciare Clusterify a queste sorgenti, creando una \emph{black-box} che trasforma i testi in un dato strutturato precedentemente definito. Clusterify nasce con \emph{TwitterReader} che, come da nome, permette di leggere i dati direttamente dalle API di Twitter. Per creare un nuovo reader, quindi, basta soddisfare trasformare l'input secondo la struttura imposta.

	La cache, invece, è una componente strutturale che viene utilizzata da tutto il progetto per evitare il replicarsi di richieste HTTP già fatte. Clusterify nasce con Redis -- \url{http://redis.io/} -- integrato, permettendo comunque di cambiare questa componente se lo sviluppatore lo desidera.

	L'ultima componente estendibile sono gli algoritmi di clustering.

	\subsection{Algoritmi di clustering}
		Un algoritmo di clustering permette di raggruppare oggetti all'interno di insiemi, mantenendo uniti gli elementi che secondo alcune regole risultano più coesi e dividendo quelli che lo sono meno.

		Poiché non esiste un algoritmo ottimo per ogni caso pensabile, Clusterify ne comprende \emph{quattro} nel core. Questi sono utili sia a rilasciare un sistema in grado di coprire la maggior parte dei casi d'uso, sia per valutare qual'è la performance migliore con l'input in questione.

		I quattro algoritmi presenti in Clusterify sono presi da scikit-learn -- \url{http://scikit-learn.org/}. Questa decisione è stata presa sia per offrire più di un algoritmo alla community, sia per valutare se la mia ipotesi iniziale circa il miglior algoritmo fosse corretta. 

		Questi algoritmi sono \emph{K-means}, \emph{Spectral}, \emph{Affinity Propagation} e \emph{Ward}.

		\subsubsection{K-means}
			\input{kmeans}

		\subsubsection{Spectral}
			\input{spectral}

		\subsubsection{Affinity Propagation}
			\input{affinity}

		\subsubsection{Ward}
			\input{ward}
		

\section{Scelta dell'algoritmo di clustering}
	\input{scelta_algoritmo}

\section{Workflow}
	Clusterify non è altro che un sistema di \emph{pipe}, dove un comando viene eseguito prendendo come input l'output del precedente.

	Per permettere che un elaborazione non ne blocchi un'altra, si è deciso di utilizzare celery -- \url{http://www.celeryproject.org/} --, un sistema di code per la gestione di \emph{task}. Così facendo, quando un utente richiede un'elaborazione, la si mette in coda e si informa l'utente che questa partirà appena possibile. Una volta terminata, l'utente riceverà una mail con la notifica di fine workflow.

	Il workflow si può suddividere in tre parti:

	\begin{enumerate}
  		\item Acquisizione dei testi
  		\item Annotazione dei testi
 		\item Clustering dei testi
	\end{enumerate} 
	
	\subsection{Acquisizione dei testi}
		La componente di acquisizione testi, come detto precedentemente, è una delle parti pluggabili di Clusterify.

		Questa fase si occupa del prendere i dati da una qualsivoglia fonte e di salvarli in modo conforme allo standard imposto. All'interno di questa componente è possibile implementare, se necessario, un livello di cache in modo da impedire di richiedere più volte le stesse informazioni.

		In Clusterify, \emph{TwitterReader} utilizza \emph{Twython}  -- \url{https://github.com/ryanmcgrath/twython} --, una libreria Python che offre un accesso facile ai dati presenti su Twitter. In altre parole, Twython offre un \emph{wrapper} alle API di Twitter, permettendo al programmatore di ottenere dati senza preoccuparsi della gestione errori, dell'aggiornamento delle API e quant'altro. 

	\subsection{Annotazioni dei testi}
		La seconda fase del workflow si occupa di annotare i testi per mezzo di dataTXT.
		
		Quest'operazione è fondamentale per un buon risultato finale in quanto si può  incorrere nel riconoscimento di entità sbagliate; oppure, nel caso contrario, ci si può imbattere nell'estrazione di poche entità ma molto precise. Avere troppe annotazioni sbagliate può condurre all'avere dei dati non veritieri; il contrario potrebbe portare a non avere nemmeno un'entità estratta per testo analizzato e quindi, in fase di clustering, i testi analizzati da cui non si estraggono entità, verrebbero persi.

		Al programmatore è quindi chiesto di scegliere molto attentamente i parametri che verranno passati all'estrattore, per non finire in nessuno dei due casi.

	\subsection{Clustering dei testi}
		L'ultima fase la si può pensare suddivisa a sua volta in due sotto fasi:

		\begin{enumerate}
  			\item Creazione matrice delle adiacenze
  			\item Esecuzione dell'algoritmo di clustering
		\end{enumerate} 

		\subsubsection{Creazione matrice di adiacenza}
			La maggior parte degli algoritmi di clustering necessita di un grafo pesato su cui operare. Questo è rappresentato per mezzo di una matrice delle adiacenze $N \times N$, dove $N$ è la cardinalità dell'insieme composto da tutte le entità, e viene costruita in questo modo:

			\begin{equation*}
				Adj(a, b) = \begin{cases} 
					rel(a,b), & \mbox{se } a \neq b \\ 
					0, & altrimenti 
				\end{cases}
			\end{equation*}
			ove $Adj$ è la nostra matrice delle adiacenze, $a$ e $b$ sono i due topic e $rel(a,b)$ è la chiamata all'API dataTXT-REL che permette di capire quanto due entità siano correlate tra loro.
			
			Si deduce che $Adj$ è una matrice speculare, con valori reali compresi tra $0$ e $1$, e che presenta $0$ sulla diagonale.

		\subsubsection{Esecuzione dell'algoritmo di clustering}
			Una volta generata la matrice delle adiacenze si può eseguire l'algoritmo di clustering precedentemente scelto. Dato l'output, i dati saranno processati nuovamente attraverso una funzione votata a due principali compiti:
			\begin{itemize}
  				\item Rendere l'output conforme al modello prestabilito
	  			\item Aggiungere informazioni ad ogni componente del cluster
 			\end{itemize} 
			
			La prima si occuperà di trasformare la struttura dei dati generata dall'algoritmo in quella stabilita da Clusterify; la seconda, invece, si occuperà di aumentare l'informazione che questa contiene.

			Ad esempio in Clusterify la seconda funzione aggiunge un dato che indica quanto una componente deve essere grande, calcolando questo dato sul numero di occorrenze che la stessa entità ha nel dataset.

\section{Frontend}
	\input{frontend}
